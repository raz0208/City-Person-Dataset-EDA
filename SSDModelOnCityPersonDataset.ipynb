{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hKXaSws-nzPAss7Kxw66KxT3yCpV6Y9X",
      "authorship_tag": "ABX9TyOjSqut3Mdjdg1euvTAfisJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/City-Person-Dataset-EDA/blob/main/SSDModelOnCityPersonDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SSD (Single Shot MultiBox Detector) Implementation on CityPerson Dataset"
      ],
      "metadata": {
        "id": "SpcMwwi47cW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Extract and read the datasets"
      ],
      "metadata": {
        "id": "68o58_lt7t7U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "q8zWx5k_7WHP"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from torchvision.models.detection.ssd import SSD300_VGG16_Weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset zip files path from Google Drive\n",
        "#gtFine = '/content/drive/MyDrive/CityPersonDataset/gtFine_trainvaltest.zip'\n",
        "gtFinePanopticParts = '/content/drive/MyDrive/CityPersonDataset/gtFinePanopticParts_trainval.zip'\n",
        "gtBbox = '/content/drive/MyDrive/CityPersonDataset/gtBbox_cityPersons_trainval.zip'\n",
        "\n",
        "#gtFine_ExtPath = '/content/CityPersonDataset/gtFine_trainvaltest'\n",
        "gtFinePano_ExtPath = '/content/CityPersonDataset/gtFinePanopticParts_trainval'\n",
        "gtBbox_ExtPath = '/content/CityPersonDataset/gtBbox_cityPersons_trainval'"
      ],
      "metadata": {
        "id": "AtIpjkyL8Hyd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting files function\n",
        "def extract_zip(file_path, extract_path):\n",
        "    if not os.path.exists(extract_path):\n",
        "        os.makedirs(extract_path)\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# Extract both datasets zip files\n",
        "#extract_zip(gtFine, gtFine_ExtPath)\n",
        "extract_zip(gtFinePanopticParts, gtFinePano_ExtPath)\n",
        "extract_zip(gtBbox, gtBbox_ExtPath)"
      ],
      "metadata": {
        "id": "XfjCgI6T8MvR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the extracted content from both datasets\n",
        "#gtFine_Files = os.listdir(gtFine_ExtPath)\n",
        "gtFinepano_Files = os.listdir(gtFinePano_ExtPath)\n",
        "gtBbox_Files = os.listdir(gtBbox_ExtPath)\n",
        "\n",
        "#gtFine_Files,\n",
        "gtFinepano_Files, gtBbox_Files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lduNoByR85Kk",
        "outputId": "c13e53be-121f-4351-c600-89ad104cf1b2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['license.txt', 'README_panopticParts.md', 'gtFinePanopticParts'],\n",
              " ['license.txt', 'README_cityPersons', 'gtBboxCityPersons'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to core folder\n",
        "#gtFine_CorePath = os.path.join(gtFine_ExtPath, 'gtFine')\n",
        "gtFinePano_CorePath = os.path.join(gtFinePano_ExtPath, 'gtFinePanopticParts')\n",
        "gtBbox_CorePath = os.path.join(gtBbox_ExtPath, 'gtBboxCityPersons')\n",
        "\n",
        "# List driectories inside core folders\n",
        "#gtFine_Dirs = os.listdir(gtFine_CorePath) if os.path.exists(gtFine_CorePath) else []\n",
        "gtFinePano_Dirs = os.listdir(gtFinePano_CorePath) if os.path.exists(gtFinePano_CorePath) else []\n",
        "gtBbox_Dirs = os.listdir(gtBbox_CorePath) if os.path.exists(gtBbox_CorePath) else []\n",
        "\n",
        "#gtFine_Dirs,\n",
        "gtFinePano_Dirs, gtBbox_Dirs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veOwW1vp9Tf8",
        "outputId": "a756575a-4cf3-484d-ca9d-ab68c647bc0b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['val', 'train'], ['val', 'train'])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the subdirectories\n",
        "subdirs = gtBbox_Dirs #[\"train\", \"val\", \"test\"]\n",
        "\n",
        "# Initialize dictionaries to store samples from each subdirectory\n",
        "#gtFine_CityFolders = {}\n",
        "gtFinePano_CityFolders = {}\n",
        "gtBbox_CityFolders = {}\n",
        "\n",
        "# Process each subdirectory\n",
        "for subdir in subdirs:\n",
        "    #gtFine_CityFolders[subdir] = os.listdir(os.path.join(gtFine_CorePath, subdir)) if subdir in gtFine_Dirs else []\n",
        "    gtFinePano_CityFolders[subdir] = os.listdir(os.path.join(gtFinePano_CorePath, subdir)) if subdir in gtFinePano_Dirs else []\n",
        "    gtBbox_CityFolders[subdir] = os.listdir(os.path.join(gtBbox_CorePath, subdir)) if subdir in gtBbox_Dirs else []\n",
        "\n",
        "# Output the first few files for each subdirectory\n",
        "#gtFine_CityFolders_Preview = {key: value[:] for key, value in gtFine_CityFolders.items()}\n",
        "gtFinePano_CityFolders_Preview = {key: value[:] for key, value in gtFinePano_CityFolders.items()}\n",
        "gtBbox_CityFolders_Preview = {key: value[:] for key, value in gtBbox_CityFolders.items()}\n",
        "\n",
        "#gtFine_CityFolders_Preview,\n",
        "gtFinePano_CityFolders_Preview, gtBbox_CityFolders_Preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIBRhfJF-cFb",
        "outputId": "5c2b34d0-18bf-4e58-bd85-23ffc3e33dea"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'val': ['lindau', 'munster', 'frankfurt'],\n",
              "  'train': ['hanover',\n",
              "   'strasbourg',\n",
              "   'cologne',\n",
              "   'bochum',\n",
              "   'jena',\n",
              "   'hamburg',\n",
              "   'monchengladbach',\n",
              "   'zurich',\n",
              "   'dusseldorf',\n",
              "   'weimar',\n",
              "   'stuttgart',\n",
              "   'erfurt',\n",
              "   'darmstadt',\n",
              "   'bremen',\n",
              "   'aachen',\n",
              "   'tubingen',\n",
              "   'ulm',\n",
              "   'krefeld']},\n",
              " {'val': ['lindau', 'munster', 'frankfurt'],\n",
              "  'train': ['hanover',\n",
              "   'strasbourg',\n",
              "   'cologne',\n",
              "   'bochum',\n",
              "   'jena',\n",
              "   'hamburg',\n",
              "   'monchengladbach',\n",
              "   'zurich',\n",
              "   'dusseldorf',\n",
              "   'weimar',\n",
              "   'stuttgart',\n",
              "   'erfurt',\n",
              "   'darmstadt',\n",
              "   'bremen',\n",
              "   'aachen',\n",
              "   'tubingen',\n",
              "   'ulm',\n",
              "   'krefeld']})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the distribution of file types in a given directory\n",
        "def count_file_types(directory):\n",
        "    if not os.path.exists(directory):\n",
        "      print(f\"Error: Directory '{directory}' not found.\")\n",
        "      return None\n",
        "\n",
        "    file_type_counts = Counter()\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            ext = os.path.splitext(file)[-1].lower()\n",
        "            file_type_counts[ext] += 1\n",
        "    return file_type_counts\n",
        "\n",
        "# # File type distribution for gtFine dataset\n",
        "# print(\"File Type Distribution in gtFine Dataset:\")\n",
        "# gtFine_file_types = count_file_types(gtFine_CorePath)\n",
        "# print(pd.DataFrame(gtFine_file_types.items(), columns=[\"File Type\", \"Count\"]))\n",
        "\n",
        "# Analyze file type distribution for gtFinePanopticParts dataset\n",
        "print(\"\\nFile Type Distribution in gtFinePanopticParts Dataset:\")\n",
        "gtFinePano_file_types = count_file_types(gtFinePano_CorePath)\n",
        "print(pd.DataFrame(gtFinePano_file_types.items(), columns=[\"File Type\", \"Count\"]))\n",
        "\n",
        "# Analyze file type distribution for gtBboxCityPersons dataset\n",
        "print(\"\\nFile Type Distribution in gtBboxCityPersons Dataset:\")\n",
        "gtBbox_file_types = count_file_types(gtBbox_CorePath)\n",
        "print(pd.DataFrame(gtBbox_file_types.items(), columns=[\"File Type\", \"Count\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vd9wuN4-nJE",
        "outputId": "aad76b7e-5b7b-4d9e-9f68-c77cf3a8ee66"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "File Type Distribution in gtFinePanopticParts Dataset:\n",
            "  File Type  Count\n",
            "0      .tif   3475\n",
            "\n",
            "File Type Distribution in gtBboxCityPersons Dataset:\n",
            "  File Type  Count\n",
            "0     .json   3475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "Gx2dIllADkZ1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CityPersonsDataset(Dataset):\n",
        "    def __init__(self, images_dir, labels_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_dir = labels_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.label_paths = []\n",
        "\n",
        "        # Get image and annotation file paths\n",
        "        for city in os.listdir(images_dir):\n",
        "            city_image_path = os.path.join(images_dir, city)\n",
        "            city_label_path = os.path.join(labels_dir, city)\n",
        "\n",
        "            if os.path.isdir(city_image_path) and os.path.isdir(city_label_path):\n",
        "                for img_name in os.listdir(city_image_path):\n",
        "                    if img_name.endswith('.tif'):\n",
        "                        img_path = os.path.join(city_image_path, img_name)\n",
        "                        label_path = os.path.join(city_label_path, img_name.replace('.tif', '.txt'))\n",
        "                        if os.path.exists(label_path):\n",
        "                            self.image_paths.append(img_path)\n",
        "                            self.label_paths.append(label_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label_path = self.label_paths[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Load bounding boxes\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        with open(label_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                data = list(map(float, line.strip().split()))\n",
        "                class_id, x, y, width, height = data\n",
        "                boxes.append([x, y, x + width, y + height])  # Convert to [x_min, y_min, x_max, y_max]\n",
        "                labels.append(int(class_id))\n",
        "\n",
        "        # Convert to tensors\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Apply transformations if provided\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image)\n",
        "            image = transformed\n",
        "\n",
        "        # Return image and target dictionary\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "9_n_rtN2DwF0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((300, 300)),  # SSD300 requires 300x300 input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "a7xKNzmND4Y8"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "train_img_dir = \"/content/CityPersonDataset/gtFinePanopticParts_trainval/gtFinePanopticParts/train\"\n",
        "train_label_dir = \"/content/CityPersonDataset/gtBbox_cityPersons_trainval/gtBboxCityPersons/train\"\n",
        "\n",
        "val_img_dir = \"/content/CityPersonDataset/gtFinePanopticParts_trainval/gtFinePanopticParts/val\"\n",
        "val_label_dir = \"/content/CityPersonDataset/gtBbox_cityPersons_trainval/gtBboxCityPersons/val\"\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = CityPersonsDataset(train_img_dir, train_label_dir, transform=transform)\n",
        "val_dataset = CityPersonsDataset(val_img_dir, val_label_dir, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Check dataset size\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "PBuWzRJkEGGc",
        "outputId": "709b9721-237d-40a3-a97f-71252ec32e66"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-936934812c19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Create DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    }
  ]
}